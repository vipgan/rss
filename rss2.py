import asyncio
import aiohttp
import logging
import re
import os
import json
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from feedparser import parse
from telegram import Bot
from telegram.error import BadRequest
from tencentcloud.common import credential
from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.tmt.v20180321 import tmt_client, models

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# é…ç½®ç»å¯¹è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent
STATUS_FILE = BASE_DIR / "rss2.json"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename=BASE_DIR / "rss2.log",
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)
# ç¿»è¯‘ä¸»é¢˜å’Œå†…å®¹
RSS_FEEDS = [
  #  'https://feeds.bbci.co.uk/news/world/rss.xml', # bbc
    'https://www3.nhk.or.jp/rss/news/cat6.xml',  # nhk
  #  'https://www.cnbc.com/id/100003114/device/rss/rss.html', # CNBC
  #  'https://feeds.a.dj.com/rss/RSSWorldNews.xml', # åå°”è¡—æ—¥æŠ¥
  #  'https://www.aljazeera.com/xml/rss/all.xml',# åŠå²›ç”µè§†å°
  #  'https://www3.nhk.or.jp/rss/news/cat5.xml',# NHK å•†ä¸š
  #  'https://www.ft.com/?format=rss', # é‡‘èæ—¶æŠ¥
  #  'http://rss.cnn.com/rss/edition.rss', # cnn
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCupvZG-5ko_eiXAupbDfxWw', # cnn
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog', # Asmongold TV
]
#ä¸»é¢˜+å†…å®¹
THIRD_RSS_FEEDS = [
  #  'https://36kr.com/feed-newsflash',
  #  'https://rss.owo.nz/10jqka/realtimenews',
]
 # ä¸»é¢˜+é¢„è§ˆ
FOURTH_RSS_FEEDS = [
    'https://blog.090227.xyz/atom.xml',
    'https://www.freedidi.com/feed',
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCUNciDq-y6I6lEQPeoP-R5A', # è‹æ’è§‚å¯Ÿ
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCXkOTZJ743JgVhJWmNV8F3Q', # å¯’åœ‹äºº
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC2r2LPbOUssIa02EbOIm7NA', # æ˜Ÿçƒç†±é»
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCF-Q1Zwyn9681F7du8DMAWg', # è¬å®—æ¡“-è€è¬ä¾†äº†
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCOSmkVK2xsihzKXQgiXPS4w', # å†å²å“¥
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCNiJNzSkfumLB7bYtXcIEmg', # çœŸçš„å¾ˆåšé€š
 #   'https://www.youtube.com/feeds/videos.xml?channel_id=UCG_gH6S-2ZUOtEw27uIS_QA', # 7Carå°ä¸ƒè»Šè§€é»
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCJ5rBA0z4WFGtUTS83sAb_A', # POP Radioè¯æ’­ç¶²
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCN0eCImZY6_OiJbo8cy5bLw', # å±ˆæ©ŸTV
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCb3TZ4SD_Ys3j4z0-8o6auA', # BBC News ä¸­æ–‡
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCiwt1aanVMoPYUt_CQYCPQg', # å…¨çƒå¤§è¦–é‡
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC000Jn3HGeQSwBuX_cLDK8Q', # æˆ‘æ˜¯æŸ³å‚‘å…‹
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFEBaHCJrHu2hzDA_69WQg', # å›½æ¼«è¯´
    'https://www.youtube.com/feeds/videos.xml?channel_id=UChJ8YKw6E1rjFHVS9vovrZw', # BNE TV - æ–°è¥¿å…°ä¸­æ–‡å›½é™…é¢‘é“

# å½±è§†
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC7Xeh7thVIgs_qfTlwC-dag', # Marc TV
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCqWNOHjgfL8ADEdXGznzwUw', # æ‚¦è€³éŸ³ä¹é…±
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCCD14H7fJQl3UZNWhYMG3Mg', # æ¸©åŸé²¤
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQO2T82PiHCYbqmCQ6QO6lw', # æœˆäº®èªª
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCKyDmY3R_xGKz8IjvbijiHA', # çŠçŠè¿½å‰§ç¤¾
    'https://www.youtube.com/feeds/videos.xml?channel_id=UClyVC2wh_2fQhU0hPdXA4rw', # çƒ­é—¨å¤é£
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UC1ISajIKfRN359MMmtckUTg', # Taiwanese Pop Mix
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFyMGc6h30NMCd6HCk0ZPA', # å“”å“©å“”å“©åŠ¨ç”»
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCQatgKoA7lylp_UzvsLCgcw', # è…¾è®¯è§†é¢‘
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCUhpu5MJQ_bjPkXO00jyxsw', # çˆ±å¥‡è‰º
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCHW6W9g2TJL2_Lf7GfoI5kg', # ç”µå½±æ”¾æ˜ å…
]

# ç¿»è¯‘ä¸»é¢˜+é“¾æ¥çš„
FIFTH_RSS_FEEDS = [
  #  'https://rsshub.app/twitter/media/elonmusk',  #elonmusk
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCupvZG-5ko_eiXAupbDfxWw', # cnn
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog', # Asmongold TV
]

# Telegramé…ç½®
TELEGRAM_BOT_TOKEN = os.getenv("RSS_TWO")  # bbc
RSS_TWO = os.getenv("RSS_TWO")
YOUTUBE_RSS = os.getenv("YOUTUBE_RSS")    # 10086
RSSTWO_TOKEN = os.getenv("YOUTUBE_RSS")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").split(",")
TENCENTCLOUD_SECRET_ID = os.getenv("TENCENTCLOUD_SECRET_ID")
TENCENTCLOUD_SECRET_KEY = os.getenv("TENCENTCLOUD_SECRET_KEY")

MAX_CONCURRENT_REQUESTS = 5
semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

def remove_html_tags(text):
    """å½»åº•ç§»é™¤HTMLæ ‡ç­¾"""
    return re.sub(r'<[^>]*>', '', text)

def escape_markdown_v2(text, exclude=None):
    """è‡ªå®šä¹‰MarkdownV2è½¬ä¹‰å‡½æ•°"""
    if exclude is None:
        exclude = []
    chars = '_*[]()~`>#+-=|{}.!'
    chars_to_escape = [c for c in chars if c not in exclude]
    pattern = re.compile(f'([{"".join(map(re.escape, chars_to_escape))}])')
    return pattern.sub(r'\\\1', text)

async def send_single_message(bot, chat_id, text, disable_web_page_preview=False):
    try:
        MAX_MESSAGE_LENGTH = 4096
        text_chunks = []
        current_chunk = []
        current_length = 0

        # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²ä¿æŒæ®µè½ç»“æ„
        paragraphs = text.split('\n\n')
        for para in paragraphs:
            para_length = len(para.encode('utf-8'))
            if current_length + para_length + 2 > MAX_MESSAGE_LENGTH:  # +2 æ˜¯æ¢è¡Œç¬¦
                text_chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(para)
            current_length += para_length + 2

        if current_chunk:
            text_chunks.append('\n\n'.join(current_chunk))

        for chunk in text_chunks:
            await bot.send_message(
                chat_id=chat_id,
                text=chunk,
                parse_mode='MarkdownV2',
                disable_web_page_preview=disable_web_page_preview
            )
    except BadRequest as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥(Markdowné”™è¯¯): {e} - æ–‡æœ¬ç‰‡æ®µ: {chunk[:200]}...")
    except Exception as e:
        logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {e}")

async def fetch_feed(session, feed_url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}
    try:
        async with semaphore:
            async with session.get(feed_url, headers=headers, timeout=40) as response:
                response.raise_for_status()
                return parse(await response.read())
    except Exception as e:
        logging.error(f"æŠ“å–å¤±è´¥ {feed_url}: {e}")
        return None

async def auto_translate_text(text):
    try:
        cred = credential.Credential(TENCENTCLOUD_SECRET_ID, TENCENTCLOUD_SECRET_KEY)
        clientProfile = ClientProfile(httpProfile=HttpProfile(endpoint="tmt.tencentcloudapi.com"))
        client = tmt_client.TmtClient(cred, "na-siliconvalley", clientProfile)

        req = models.TextTranslateRequest()
        req.SourceText = remove_html_tags(text)  # ç¿»è¯‘å‰å…ˆç§»é™¤HTML
        req.Source = "auto"
        req.Target = "zh"
        req.ProjectId = 0

        return client.TextTranslate(req).TargetText
    except Exception as e:
        logging.error(f"ç¿»è¯‘é”™è¯¯: {e}")
        return text

def load_status():
    try:
        with open(STATUS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_status(status):
    try:
        with open(STATUS_FILE, "w", encoding="utf-8") as f:
            json.dump(status, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logging.error(f"çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")

def get_entry_identifier(entry):
    """è·å–æ¡ç›®å”¯ä¸€æ ‡è¯†"""
    if hasattr(entry, 'guid') and entry.guid:
        return entry.guid
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6]).isoformat()
    if hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        return datetime(*entry.pubDate_parsed[:6]).isoformat()
    return f"{entry.get('title', '')}-{entry.get('link', '')}"

def get_entry_timestamp(entry):
    """è·å–æ ‡å‡†åŒ–æ—¶é—´æˆ³"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6])
    if hasattr(entry, 'pubDate_parsed') and entry.pubDate_parsed:
        return datetime(*entry.pubDate_parsed[:6])
    return datetime.now()

async def process_feed(session, feed_url, status, bot, translate=True):
    feed_data = await fetch_feed(session, feed_url)
    if not feed_data or not feed_data.entries:
        return ""

    # çŠ¶æ€å¤„ç†
    last_status = status.get(feed_url, {})
    last_identifier = last_status.get('identifier')
    last_timestamp = datetime.fromisoformat(last_status.get('timestamp')) if last_status.get('timestamp') else None

    # æŒ‰æ—¶é—´æ’åº
    sorted_entries = sorted(feed_data.entries,
                          key=lambda x: get_entry_timestamp(x),
                          reverse=True)

    new_entries = []
    current_latest = None

    for entry in sorted_entries:
        entry_time = get_entry_timestamp(entry)
        identifier = get_entry_identifier(entry)

        if last_identifier and identifier == last_identifier:
            break
        if last_timestamp and entry_time <= last_timestamp:
            break

        new_entries.append(entry)
        if not current_latest or entry_time > get_entry_timestamp(current_latest):
            current_latest = entry

    if not new_entries:
        return ""

    # æ›´æ–°çŠ¶æ€
    if current_latest:
        status[feed_url] = {
            "identifier": get_entry_identifier(current_latest),
            "timestamp": get_entry_timestamp(current_latest).isoformat()
        }

    # å¤„ç†æ¶ˆæ¯
    merged_message = ""
    source_name = feed_data.feed.get('title', feed_url)
    # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
    for idx, entry in enumerate(reversed(new_entries), start=1):
        # åŸå§‹å†…å®¹å¤„ç†
        raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
        raw_summary = remove_html_tags(getattr(entry, 'summary', "æš‚æ— ç®€ä»‹"))
        raw_url = entry.link

        # ç¿»è¯‘å¤„ç†
        if translate:
            translated_subject = await auto_translate_text(raw_subject)
            translated_summary = await auto_translate_text(raw_summary)
        else:
            translated_subject = raw_subject
            translated_summary = raw_summary

        # Markdownè½¬ä¹‰
        safe_subject = escape_markdown_v2(translated_subject, exclude=['*'])
        safe_summary = escape_markdown_v2(translated_summary)
        safe_source = escape_markdown_v2(source_name, exclude=['[', ']'])
        safe_url = escape_markdown_v2(raw_url)

        # æ„å»ºæ¶ˆæ¯
        message = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
        merged_message += message + "\n\n"
    merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"
    return merged_message

async def process_third_feed(session, feed_url, status, bot):
    feed_data = await fetch_feed(session, feed_url)
    if not feed_data or not feed_data.entries:
        return ""

    last_status = status.get(feed_url, {})
    last_identifier = last_status.get('identifier')
    last_timestamp = datetime.fromisoformat(last_status.get('timestamp')) if last_status.get('timestamp') else None

    sorted_entries = sorted(feed_data.entries,
                          key=lambda x: get_entry_timestamp(x),
                          reverse=True)

    new_entries = []
    current_latest = None

    for entry in sorted_entries:
        entry_time = get_entry_timestamp(entry)
        identifier = get_entry_identifier(entry)

        if last_identifier and identifier == last_identifier:
            break
        if last_timestamp and entry_time <= last_timestamp:
            break

        new_entries.append(entry)
        if not current_latest or entry_time > get_entry_timestamp(current_latest):
            current_latest = entry

    if not new_entries:
        return ""

    if current_latest:
        status[feed_url] = {
            "identifier": get_entry_identifier(current_latest),
            "timestamp": get_entry_timestamp(current_latest).isoformat()
        }

    merged_message = ""
    source_name = feed_data.feed.get('title', feed_url)
    # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
    for idx, entry in enumerate(reversed(new_entries), start=1):
        # å†…å®¹å¤„ç†
        raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
        raw_summary = remove_html_tags(getattr(entry, 'summary', "æš‚æ— ç®€ä»‹"))
        raw_url = entry.link

        # Markdownè½¬ä¹‰
        safe_subject = escape_markdown_v2(raw_subject, exclude=['*'])
        safe_summary = escape_markdown_v2(raw_summary)
        safe_source = escape_markdown_v2(source_name, exclude=['[', ']'])
        safe_url = escape_markdown_v2(raw_url)

        # æ¶ˆæ¯æ„å»º
        message_content = f"*{safe_subject}*\n{safe_summary}\n[{safe_source}]({safe_url})"
        message_bytes = message_content.encode('utf-8')

        if len(message_bytes) <= 555:
            merged_message += message_content + "\n\n"
        else:
            title_link = f"*{safe_subject}*\n[{safe_source}]({safe_url})"
            merged_message += title_link + "\n\n"
    merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"
    return merged_message

async def process_fourth_feed(session, feed_url, status, bot):
    feed_data = await fetch_feed(session, feed_url)
    if not feed_data or not feed_data.entries:
        return ""

    last_status = status.get(feed_url, {})
    last_identifier = last_status.get('identifier')
    last_timestamp = datetime.fromisoformat(last_status.get('timestamp')) if last_status.get('timestamp') else None

    sorted_entries = sorted(feed_data.entries,
                          key=lambda x: get_entry_timestamp(x),
                          reverse=True)

    new_entries = []
    current_latest = None

    for entry in sorted_entries:
        entry_time = get_entry_timestamp(entry)
        identifier = get_entry_identifier(entry)

        if last_identifier and identifier == last_identifier:
            break
        if last_timestamp and entry_time <= last_timestamp:
            break

        new_entries.append(entry)
        if not current_latest or entry_time > get_entry_timestamp(current_latest):
            current_latest = entry

    if not new_entries:
        return ""

    if current_latest:
        status[feed_url] = {
            "identifier": get_entry_identifier(current_latest),
            "timestamp": get_entry_timestamp(current_latest).isoformat()
        }

    merged_message = ""
    source_name = feed_data.feed.get('title', feed_url)
    feed_title = f"**{escape_markdown_v2(source_name, exclude=['*'])}**"  # è½¬ä¹‰å¹¶åŠ ç²—æ ‡é¢˜

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    merged_message += f"ğŸ“¢ *{feed_title}*\n\n"

    # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
    for idx, entry in enumerate(reversed(new_entries), start=1):
        # å†…å®¹å¤„ç†
        raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
        raw_url = entry.link

        clean_subject = re.sub(r'[^\w\s\u4e00-\u9fa5.,!?;:"\'()\-]+', '', raw_subject).strip()
        # Markdownè½¬ä¹‰
        safe_subject = escape_markdown_v2(clean_subject, exclude=['*'])
        safe_url = escape_markdown_v2(raw_url)

        # æ„å»ºæ¶ˆæ¯ï¼Œæ·»åŠ åºå·
        merged_message += f"*{safe_subject}*\nğŸ”— {safe_url}\n\n"
    merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"
    return merged_message

async def process_fifth_feed(session, feed_url, status, bot, translate=True):
    feed_data = await fetch_feed(session, feed_url)
    if not feed_data or not feed_data.entries:
        return ""

    # çŠ¶æ€å¤„ç†
    last_status = status.get(feed_url, {})
    last_identifier = last_status.get('identifier')
    last_timestamp = datetime.fromisoformat(last_status.get('timestamp')) if last_status.get('timestamp') else None

    # æŒ‰æ—¶é—´æ’åº
    sorted_entries = sorted(feed_data.entries,
                          key=lambda x: get_entry_timestamp(x),
                          reverse=True)

    new_entries = []
    current_latest = None

    for entry in sorted_entries:
        entry_time = get_entry_timestamp(entry)
        identifier = get_entry_identifier(entry)

        if last_identifier and identifier == last_identifier:
            break
        if last_timestamp and entry_time <= last_timestamp:
            break

        new_entries.append(entry)
        if not current_latest or entry_time > get_entry_timestamp(current_latest):
            current_latest = entry

    if not new_entries:
        return ""

    # æ›´æ–°çŠ¶æ€
    if current_latest:
        status[feed_url] = {
            "identifier": get_entry_identifier(current_latest),
            "timestamp": get_entry_timestamp(current_latest).isoformat()
        }

    # å¤„ç†æ¶ˆæ¯
    merged_message = ""
    source_name = feed_data.feed.get('title', feed_url)
    feed_title = f"**{escape_markdown_v2(source_name, exclude=['*'])}**"  # è½¬ä¹‰å¹¶åŠ ç²—æ ‡é¢˜

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    merged_message += f"ğŸ“¢ *{feed_title}*\n\n"
    # éå†æ–°æ¡ç›®ï¼Œæ·»åŠ åºå·
    for idx, entry in enumerate(reversed(new_entries), start=1):
        # åŸå§‹å†…å®¹å¤„ç†
        raw_subject = remove_html_tags(entry.title or "æ— æ ‡é¢˜")
        raw_url = entry.link

        # ç¿»è¯‘å¤„ç†
        if translate:
            translated_subject = await auto_translate_text(raw_subject)
        else:
            translated_subject = raw_subject

        # Markdownè½¬ä¹‰
        safe_subject = escape_markdown_v2(translated_subject, exclude=['*'])
        safe_source = escape_markdown_v2(source_name, exclude=['[', ']'])
        safe_url = escape_markdown_v2(raw_url)

        # æ„å»ºæ¶ˆæ¯, åªå‘é€ä¸»é¢˜å’Œé“¾æ¥
        message = f"*{safe_subject}*\nğŸ”— {safe_url}"
        merged_message += message + "\n\n"
    merged_message += f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹"
    return merged_message

async def main():
    async with aiohttp.ClientSession() as session:
        bot = Bot(token=TELEGRAM_BOT_TOKEN)
        third_bot = Bot(token=RSS_TWO)
        fourth_bot = Bot(token=YOUTUBE_RSS)
        fifth_bot = Bot(token=RSSTWO_TOKEN) 
        status = load_status()

        # å¤„ç†ç¬¬ä¸€ç±»æº
        for url in RSS_FEEDS:
            if message := await process_feed(session, url, status, bot):
                await send_single_message(bot, TELEGRAM_CHAT_ID[0], message, True)

        # å¤„ç†ç¬¬ä¸‰ç±»æº
        for url in THIRD_RSS_FEEDS:
            if message := await process_third_feed(session, url, status, third_bot):
                await send_single_message(third_bot, TELEGRAM_CHAT_ID[0], message, True)

        # å¤„ç†ç¬¬å››ç±»æº
        for url in FOURTH_RSS_FEEDS:
            if message := await process_fourth_feed(session, url, status, fourth_bot):
                await send_single_message(fourth_bot, TELEGRAM_CHAT_ID[0], message)
        
        # å¤„ç†ç¬¬äº”ç±»æº
        for url in FIFTH_RSS_FEEDS:
            if message := await process_fifth_feed(session, url, status, fifth_bot):
                await send_single_message(fifth_bot, TELEGRAM_CHAT_ID[0], message, False)  # æ ¹æ®éœ€è¦è°ƒæ•´Trueä¸æµè§ˆ

        save_status(status)

if __name__ == "__main__":
    asyncio.run(main())
