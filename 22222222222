import asyncio
import aiohttp
import logging
import json
import os
import re
from datetime import datetime
from feedparser import parse
from telegram import Bot
from telegram.constants import ParseMode
from telegram.helpers import escape_markdown
from dotenv import load_dotenv
from aiohttp import ClientTimeout

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®RSSæº
import asyncio
import aiohttp
import logging
import json
import os
import re
from datetime import datetime
from feedparser import parse
from telegram import Bot
from telegram.constants import ParseMode
from telegram.helpers import escape_markdown
from dotenv import load_dotenv
from aiohttp import ClientTimeout

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®RSSæº
RSS_FEEDS = [   
   # 'https://blog.090227.xyz/atom.xml',
   # 'https://www.freedidi.com/feed',
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
]

SECOND_RSS_FEEDS = [
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCUNciDq-y6I6lEQPeoP-R5A', # è‹æ’è§‚å¯Ÿ
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCXkOTZJ743JgVhJWmNV8F3Q', # å¯’åœ‹äºº
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC2r2LPbOUssIa02EbOIm7NA', # æ˜Ÿçƒç†±é»
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCF-Q1Zwyn9681F7du8DMAWg', # è¬å®—æ¡“-è€è¬ä¾†äº†
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCOSmkVK2xsihzKXQgiXPS4w', # å†å²å“¥
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCSYBgX9pWGiUAcBxjnj6JCQ', # éƒ­æ­£äº®é »é“
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCNiJNzSkfumLB7bYtXcIEmg', # çœŸçš„å¾ˆåšé€š
 #   'https://www.youtube.com/feeds/videos.xml?channel_id=UCG_gH6S-2ZUOtEw27uIS_QA', # 7Carå°ä¸ƒè»Šè§€é»
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCJ5rBA0z4WFGtUTS83sAb_A', # POP Radioè¯æ’­ç¶²
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog', # Asmongold TV
    'https://rss.penggan.us.kg/rss/4734eed5ffb55689bfe8ebc4f55e63bd_chinese_simplified', # Asmongold TV
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCN0eCImZY6_OiJbo8cy5bLw', # å±ˆæ©ŸTV
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCb3TZ4SD_Ys3j4z0-8o6auA', # BBC News ä¸­æ–‡
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCiwt1aanVMoPYUt_CQYCPQg', # å…¨çƒå¤§è¦–é‡
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC000Jn3HGeQSwBuX_cLDK8Q', # æˆ‘æ˜¯æŸ³å‚‘å…‹
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFEBaHCJrHu2hzDA_69WQg', # å›½æ¼«è¯´
    'https://www.youtube.com/feeds/videos.xml?channel_id=UChJ8YKw6E1rjFHVS9vovrZw', # BNE TV - æ–°è¥¿å…°ä¸­æ–‡å›½é™…é¢‘é“
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCXk0rwHPG9eGV8SaF2p8KUQ', # çƒé´‰ç¬‘ç¬‘

# å½±è§†
    'https://www.youtube.com/feeds/videos.xml?channel_id=UC7Xeh7thVIgs_qfTlwC-dag', # Marc TV
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCqWNOHjgfL8ADEdXGznzwUw', # æ‚¦è€³éŸ³ä¹é…±
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCCD14H7fJQl3UZNWhYMG3Mg', # æ¸©åŸé²¤
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCQO2T82PiHCYbqmCQ6QO6lw', # æœˆäº®èªª
   # 'https://www.youtube.com/feeds/videos.xml?channel_id=UCKyDmY3R_xGKz8IjvbijiHA', # çŠçŠè¿½å‰§ç¤¾
    'https://www.youtube.com/feeds/videos.xml?channel_id=UClyVC2wh_2fQhU0hPdXA4rw', # çƒ­é—¨å¤é£
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UC1ISajIKfRN359MMmtckUTg', # Taiwanese Pop Mix
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCQFyMGc6h30NMCd6HCk0ZPA', # å“”å“©å“”å“©åŠ¨ç”»
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCQatgKoA7lylp_UzvsLCgcw', # è…¾è®¯è§†é¢‘
  #  'https://www.youtube.com/feeds/videos.xml?channel_id=UCUhpu5MJQ_bjPkXO00jyxsw', # çˆ±å¥‡è‰º
    'https://www.youtube.com/feeds/videos.xml?channel_id=UCHW6W9g2TJL2_Lf7GfoI5kg', # ç”µå½±æ”¾æ˜ å…
]

# æ–‡ä»¶è·¯å¾„é…ç½®
script_dir = os.path.dirname(os.path.abspath(__file__))
RSS_STATUS_FILE = os.path.join(script_dir, 'rss2.json')
log_path = os.path.join(script_dir, 'rss2.log')

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_path),
        logging.StreamHandler()
    ]
)

def load_rss_status():
    """åŠ è½½RSSå¤„ç†çŠ¶æ€"""
    try:
        with open(RSS_STATUS_FILE, 'r') as f:
            return json.loads(f.read())
    except FileNotFoundError:
        return {}
    except json.JSONDecodeError:
        return {}

def save_rss_status(status):
    """ä¿å­˜RSSå¤„ç†çŠ¶æ€"""
    with open(RSS_STATUS_FILE, 'w') as f:
        json.dump(status, f, indent=2, ensure_ascii=False)

def parse_datetime(pub_date):
    """è§£ææ—¥æœŸæ—¶é—´"""
    try:
        return datetime(*pub_date[:6]) if pub_date else datetime.now()
    except TypeError:
        return datetime.now()

def prepare_content(text):
    """å¤„ç†æ–‡æœ¬å†…å®¹ä¸ºMarkdown V2å®‰å…¨æ ¼å¼"""
    # ç§»é™¤HTMLæ ‡ç­¾
    clean_text = re.sub(r'<[^>]+>', '', text)
    # å®Œæ•´è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦
    return escape_markdown(clean_text, version=2)

def prepare_url(url):
    """å¤„ç†URLä¸ºMarkdown V2å®‰å…¨æ ¼å¼"""
    # è½¬ä¹‰URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆä¸»è¦å¤„ç†æ‹¬å·ï¼‰
    return url.replace('(', '\\(').replace(')', '\\)')

async def process_feed(session, feed_url, status, bot, chat_ids, max_retries=3):
    """å¤„ç†å•ä¸ªRSSæº"""
    for attempt in range(max_retries):
        try:
            async with session.get(feed_url, timeout=30) as response:
                if response.status != 200:
                    logging.warning(f"è¯·æ±‚å¤±è´¥: {feed_url} çŠ¶æ€ç  {response.status}")
                    return

                feed = parse(await response.text())
                if not feed.entries:
                    logging.warning(f"æ— æ•ˆçš„RSSæº: {feed_url}")
                    return

                # å¤„ç†æºä¿¡æ¯
                feed_title = prepare_content(feed.feed.get('title', 'æœªçŸ¥æ¥æº'))
                entries = []
                
                # è§£ææ¡ç›®
                for entry in feed.entries:
                    try:
                        pub_date = parse_datetime(entry.get('published_parsed'))
                        entries.append({
                            'guid': entry.get('id', entry.link),
                            'pubdate': pub_date,
                            'title': prepare_content(entry.title),
                            'link': prepare_url(entry.link)  # å¤„ç†URLè½¬ä¹‰
                        })
                    except Exception as e:
                        logging.warning(f"æ¡ç›®è§£æå¤±è´¥: {e}")
                        continue

                # æŒ‰æ—¶é—´æ’åº
                entries.sort(key=lambda x: x['pubdate'], reverse=True)
                
                # è·å–ä¸Šæ¬¡çŠ¶æ€
                last_status = status.get(feed_url, {})
                last_guid = last_status.get('guid')
                last_date = datetime.fromisoformat(last_status['pubdate']) if 'pubdate' in last_status else None

                # ç­›é€‰æ–°æ¡ç›®
                new_entries = []
                for entry in entries:
                    if entry['guid'] == last_guid or (last_date and entry['pubdate'] <= last_date):
                        break
                    new_entries.append(entry)

                # å‘é€æ›´æ–°é€šçŸ¥
                if new_entries:
                    # æ„å»ºMarkdownæ¶ˆæ¯
                    message_lines = [
                        f"*{feed_title}*",
                        f"ğŸ“¢ å‘ç°{len(new_entries)}æ¡æ›´æ–°",
                        "="*30
                    ]
                    
                    for idx, entry in enumerate(reversed(new_entries), 1):
                        message_lines.append(
                            f"{idx}\\. [{entry['title']}]({entry['link']})"
                        )

                    # å‘é€åˆ°æ‰€æœ‰æŒ‡å®šèŠå¤©
                    for chat_id in chat_ids:
                        await bot.send_message(
                            chat_id=chat_id,
                            text="\n".join(message_lines),
                            parse_mode=ParseMode.MARKDOWN_V2,
                            disable_web_page_preview=True
                        )
                        await asyncio.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶

                    # æ›´æ–°çŠ¶æ€
                    status[feed_url] = {
                        'guid': entries[0]['guid'],
                        'pubdate': entries[0]['pubdate'].isoformat()
                    }
                    save_rss_status(status)
                return

        except Exception as e:
            logging.error(f"å¤„ç†æºæ—¶å‘ç”Ÿé”™è¯¯ {feed_url} (å°è¯• {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(5 * (attempt + 1))

    logging.error(f"å¤„ç†æº {feed_url} å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

async def process_feed_with_semaphore(semaphore, session, feed_url, status, bot, chat_ids):
    async with semaphore:
        await process_feed(session, feed_url, status, bot, chat_ids)

async def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–é…ç½®
    RSS_TOKEN = os.getenv("RSS_TOKEN")
    YOUTUBE_RSS = os.getenv("YOUTUBE_RSS")
    CHAT_IDS = os.getenv("TELEGRAM_CHAT_ID", "").split(",")

    if not RSS_TOKEN or not YOUTUBE_RSS:
        logging.error("ç¼ºå°‘æœºå™¨äººTokené…ç½®")
        return

    # åˆå§‹åŒ–æœºå™¨äºº
    main_bot = Bot(token=RSS_TOKEN)
    second_bot = Bot(token=YOUTUBE_RSS)
    status = load_rss_status()

    # åˆ›å»ºå¸¦å¹¶å‘é™åˆ¶çš„HTTPä¼šè¯
    semaphore = asyncio.Semaphore(5)
    timeout = ClientTimeout(total=60)
    
    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(limit=100),
        timeout=timeout
    ) as session:
        tasks = []
        
        # æ·»åŠ ä¸»RSSæºä»»åŠ¡
        for feed in RSS_FEEDS:
            tasks.append(
                process_feed_with_semaphore(semaphore, session, feed, status, main_bot, CHAT_IDS)
            )
            await asyncio.sleep(0.5)
        
        # æ·»åŠ æ¬¡è¦RSSæºä»»åŠ¡
        for feed in SECOND_RSS_FEEDS:
            tasks.append(
                process_feed_with_semaphore(semaphore, session, feed, status, second_bot, CHAT_IDS)
            )
            await asyncio.sleep(0.5)
        
        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())

# æ–‡ä»¶è·¯å¾„é…ç½®
script_dir = os.path.dirname(os.path.abspath(__file__))
RSS_STATUS_FILE = os.path.join(script_dir, 'rss2.json')
log_path = os.path.join(script_dir, 'rss2.log')

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_path),
        logging.StreamHandler()
    ]
)

def load_rss_status():
    """åŠ è½½RSSå¤„ç†çŠ¶æ€"""
    try:
        with open(RSS_STATUS_FILE, 'r') as f:
            return json.loads(f.read())
    except FileNotFoundError:
        return {}
    except json.JSONDecodeError:
        return {}

def save_rss_status(status):
    """ä¿å­˜RSSå¤„ç†çŠ¶æ€"""
    with open(RSS_STATUS_FILE, 'w') as f:
        json.dump(status, f, indent=2, ensure_ascii=False)

def parse_datetime(pub_date):
    """è§£ææ—¥æœŸæ—¶é—´"""
    try:
        return datetime(*pub_date[:6]) if pub_date else datetime.now()
    except TypeError:
        return datetime.now()

def prepare_content(text):
    """å¤„ç†æ–‡æœ¬å†…å®¹ä¸ºMarkdown V2å®‰å…¨æ ¼å¼"""
    # ç§»é™¤HTMLæ ‡ç­¾
    clean_text = re.sub(r'<[^>]+>', '', text)
    # å®Œæ•´è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦
    return escape_markdown(clean_text, version=2)

def prepare_url(url):
    """å¤„ç†URLä¸ºMarkdown V2å®‰å…¨æ ¼å¼"""
    # è½¬ä¹‰URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆä¸»è¦å¤„ç†æ‹¬å·ï¼‰
    return url.replace('(', '\\(').replace(')', '\\)')

async def process_feed(session, feed_url, status, bot, chat_ids, max_retries=3):
    """å¤„ç†å•ä¸ªRSSæº"""
    for attempt in range(max_retries):
        try:
            async with session.get(feed_url, timeout=30) as response:
                if response.status != 200:
                    logging.warning(f"è¯·æ±‚å¤±è´¥: {feed_url} çŠ¶æ€ç  {response.status}")
                    return

                feed = parse(await response.text())
                if not feed.entries:
                    logging.warning(f"æ— æ•ˆçš„RSSæº: {feed_url}")
                    return

                # å¤„ç†æºä¿¡æ¯
                feed_title = prepare_content(feed.feed.get('title', 'æœªçŸ¥æ¥æº'))
                entries = []
                
                # è§£ææ¡ç›®
                for entry in feed.entries:
                    try:
                        pub_date = parse_datetime(entry.get('published_parsed'))
                        entries.append({
                            'guid': entry.get('id', entry.link),
                            'pubdate': pub_date,
                            'title': prepare_content(entry.title),
                            'link': prepare_url(entry.link)  # å¤„ç†URLè½¬ä¹‰
                        })
                    except Exception as e:
                        logging.warning(f"æ¡ç›®è§£æå¤±è´¥: {e}")
                        continue

                # æŒ‰æ—¶é—´æ’åº
                entries.sort(key=lambda x: x['pubdate'], reverse=True)
                
                # è·å–ä¸Šæ¬¡çŠ¶æ€
                last_status = status.get(feed_url, {})
                last_guid = last_status.get('guid')
                last_date = datetime.fromisoformat(last_status['pubdate']) if 'pubdate' in last_status else None

                # ç­›é€‰æ–°æ¡ç›®
                new_entries = []
                for entry in entries:
                    if entry['guid'] == last_guid or (last_date and entry['pubdate'] <= last_date):
                        break
                    new_entries.append(entry)

                # å‘é€æ›´æ–°é€šçŸ¥
                if new_entries:
                    # æ„å»ºMarkdownæ¶ˆæ¯
                    message_lines = [
                        f"*{feed_title}*",
                        f"ğŸ“¢ å‘ç°{len(new_entries)}æ¡æ›´æ–°",
                        "="*30
                    ]
                    
                    for idx, entry in enumerate(reversed(new_entries), 1):
                        message_lines.append(
                            f"{idx}\\. [{entry['title']}]({entry['link']})"
                        )

                    # å‘é€åˆ°æ‰€æœ‰æŒ‡å®šèŠå¤©
                    for chat_id in chat_ids:
                        await bot.send_message(
                            chat_id=chat_id,
                            text="\n".join(message_lines),
                            parse_mode=ParseMode.MARKDOWN_V2,
                            disable_web_page_preview=True
                        )
                        await asyncio.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶

                    # æ›´æ–°çŠ¶æ€
                    status[feed_url] = {
                        'guid': entries[0]['guid'],
                        'pubdate': entries[0]['pubdate'].isoformat()
                    }
                    save_rss_status(status)
                return

        except Exception as e:
            logging.error(f"å¤„ç†æºæ—¶å‘ç”Ÿé”™è¯¯ {feed_url} (å°è¯• {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(5 * (attempt + 1))

    logging.error(f"å¤„ç†æº {feed_url} å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

async def process_feed_with_semaphore(semaphore, session, feed_url, status, bot, chat_ids):
    async with semaphore:
        await process_feed(session, feed_url, status, bot, chat_ids)

async def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–é…ç½®
    RSS_TOKEN = os.getenv("RSS_TOKEN")
    YOUTUBE_RSS = os.getenv("YOUTUBE_RSS")
    CHAT_IDS = os.getenv("TELEGRAM_CHAT_ID", "").split(",")

    if not RSS_TOKEN or not YOUTUBE_RSS:
        logging.error("ç¼ºå°‘æœºå™¨äººTokené…ç½®")
        return

    # åˆå§‹åŒ–æœºå™¨äºº
    main_bot = Bot(token=RSS_TOKEN)
    second_bot = Bot(token=YOUTUBE_RSS)
    status = load_rss_status()

    # åˆ›å»ºå¸¦å¹¶å‘é™åˆ¶çš„HTTPä¼šè¯
    semaphore = asyncio.Semaphore(5)
    timeout = ClientTimeout(total=60)
    
    async with aiohttp.ClientSession(
        connector=aiohttp.TCPConnector(limit=100),
        timeout=timeout
    ) as session:
        tasks = []
        
        # æ·»åŠ ä¸»RSSæºä»»åŠ¡
        for feed in RSS_FEEDS:
            tasks.append(
                process_feed_with_semaphore(semaphore, session, feed, status, main_bot, CHAT_IDS)
            )
            await asyncio.sleep(0.5)
        
        # æ·»åŠ æ¬¡è¦RSSæºä»»åŠ¡
        for feed in SECOND_RSS_FEEDS:
            tasks.append(
                process_feed_with_semaphore(semaphore, session, feed, status, second_bot, CHAT_IDS)
            )
            await asyncio.sleep(0.5)
        
        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())