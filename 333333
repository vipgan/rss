import asyncio
import aiohttp
import logging
import json
import os
import re
from datetime import datetime
from feedparser import parse
from telegram import Bot
from dotenv import load_dotenv
from aiohttp import ClientTimeout

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®RSSæº
RSS_FEEDS = [   
    'https://blog.090227.xyz/atom.xml',
    'https://www.freedidi.com/feed',
   # 'https://www..com/feeds/videos.xml?channel_id=UCvijahEyGtvMpmMHBu4FS2w', # é›¶åº¦è§£è¯´
   # 'https://www..com/feeds/videos.xml?channel_id=UC96OvMh0Mb_3NmuE8Dpu7Gg', # ææœºé›¶è·ç¦»
   # 'https://www..com/feeds/videos.xml?channel_id=UCQoagx4VHBw3HkAyzvKEEBA', # ç§‘æŠ€å…±äº«
   # 'https://www..com/feeds/videos.xml?channel_id=UCbCCUH8S3yhlm7__rhxR2QQ', # ä¸è‰¯æ—
   # 'https://www..com/feeds/videos.xml?channel_id=UCMtXiCoKFrc2ovAGc1eywDg', # ä¸€ä¼‘
   # 'https://www..com/feeds/videos.xml?channel_id=UCii04BCvYIdQvshrdNDAcww', # æ‚Ÿç©ºçš„æ—¥å¸¸
   # 'https://www..com/feeds/videos.xml?channel_id=UCJMEiNh1HvpopPU3n9vJsMQ', # ç†ç§‘ç”·å£«
   # 'https://www..com/feeds/videos.xml?channel_id=UCYjB6uufPeHSwuHs8wovLjg', # ä¸­æŒ‡é€š
   # 'https://www..com/feeds/videos.xml?channel_id=UCSs4A6HYKmHA2MG_0z-F0xw', # ææ°¸ä¹è€å¸ˆ
   # 'https://www..com/feeds/videos.xml?channel_id=UCZDgXi7VpKhBJxsPuZcBpgA', # å¯æ©KeEn
   # 'https://www..com/feeds/videos.xml?channel_id=UCxukdnZiXnTFvjF5B5dvJ5w', # ç”¬å“¥ä¾ƒä¾ƒä¾ƒygkkk
   # 'https://www..com/feeds/videos.xml?channel_id=UCUfT9BAofYBKUTiEVrgYGZw', # ç§‘æŠ€åˆ†äº«
   # 'https://www..com/feeds/videos.xml?channel_id=UC51FT5EeNPiiQzatlA2RlRA', # ä¹Œå®¢wuke
   # 'https://www..com/feeds/videos.xml?channel_id=UCDD8WJ7Il3zWBgEYBUtc9xQ', # jack stone
   # 'https://www..com/feeds/videos.xml?channel_id=UCWurUlxgm7YJPPggDz9YJjw', # ä¸€ç“¶å¥¶æ²¹
   # 'https://www..com/feeds/videos.xml?channel_id=UCvENMyIFurJi_SrnbnbyiZw', # é…·å‹ç¤¾
   # 'https://www..com/feeds/videos.xml?channel_id=UCmhbF9emhHa-oZPiBfcLFaQ', # WenWeekly
   # 'https://www..com/feeds/videos.xml?channel_id=UC3BNSKOaphlEoK4L7QTlpbA', # ä¸­å¤–è§‚å¯Ÿ
]

SECOND_RSS_FEEDS = [

 #   'https://www..com/feeds/videos.xml?channel_id=UCG_gH6S-2ZUOtEw27uIS_QA', # 7Carå°ä¸ƒè»Šè§€é»
  #  'https://www..com/feeds/videos.xml?channel_id=UCJ5rBA0z4WFGtUTS83sAb_A', # POP Radioè¯æ’­ç¶²
  #  'https://www..com/feeds/videos.xml?channel_id=UCQeRaTukNYft1_6AZPACnog', # Asmongold TV
    'https://www.freedidi.com/feed',
]

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))
# å®šä¹‰ rss2.json æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
RSS_STATUS_FILE = os.path.join(script_dir, 'rss2.json')
# é…ç½®æ—¥å¿—
log_path = os.path.join(script_dir, 'rss2.log')

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_path),  # ä½¿ç”¨ç»å¯¹è·¯å¾„
        logging.StreamHandler()
    ]
)

def load_rss_status():
    """åŠ è½½å¤„ç†çŠ¶æ€ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        with open(RSS_STATUS_FILE, 'r') as f:  # ä½¿ç”¨ç»å¯¹è·¯å¾„
            return json.loads(f.read())
    except FileNotFoundError:
        logging.info("çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
        return {}
    except json.JSONDecodeError:
        logging.warning("çŠ¶æ€æ–‡ä»¶æŸåï¼Œé‡ç½®ä¸ºé»˜è®¤çŠ¶æ€")
        return {}

def save_rss_status(status):
    """ä¿å­˜å¤„ç†çŠ¶æ€ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    with open(RSS_STATUS_FILE, 'w') as f:  # ä½¿ç”¨ç»å¯¹è·¯å¾„
        json.dump(status, f, indent=2, ensure_ascii=False)

def clean_title(title):
    """æ¸…ç†æ ‡é¢˜ç‰¹æ®Šå­—ç¬¦"""
    return re.sub(r'[^\w\s\u4e00-\u9fa5.,!?;:"\'()\-]+', '', title).strip()

def parse_datetime(pub_date):
    """è§£æå‘å¸ƒæ—¶é—´"""
    try:
        return datetime(*pub_date[:6]) if pub_date else datetime.now()
    except TypeError:
        return datetime.now()

async def process_feed(session, feed_url, status, bot, chat_ids, max_retries=3):
    """å¤„ç†å•ä¸ªRSSæº, å¸¦æœ‰é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            # è·å–RSSå†…å®¹
            async with session.get(feed_url, timeout=30) as response:
                if response.status != 200:
                    logging.warning(f"è¯·æ±‚å¤±è´¥: {feed_url} çŠ¶æ€ç  {response.status}")
                    return

                feed = parse(await response.text())
                if not feed.entries:
                    logging.warning(f"æ— æ•ˆçš„RSSæº: {feed_url}")
                    return

                # è§£ææ¡ç›®
                feed_title = feed.feed.get('title', 'æœªçŸ¥æ¥æº').strip()
                entries = []
                for entry in feed.entries:
                    try:
                        pub_date = parse_datetime(entry.get('published_parsed'))
                        entries.append({
                            'guid': entry.get('id', entry.link),
                            'pubdate': pub_date,
                            'title': clean_title(entry.title),
                            'link': entry.link
                        })
                    except Exception as e:
                        logging.warning(f"æ¡ç›®è§£æå¤±è´¥: {e}")
                        continue

                # æ’åºå’Œç­›é€‰æ–°æ¡ç›®
                entries.sort(key=lambda x: x['pubdate'], reverse=True)
                last_status = status.get(feed_url, {})
                last_guid = last_status.get('guid')
                last_date = datetime.fromisoformat(last_status['pubdate']) if 'pubdate' in last_status else None

                new_entries = []
                for entry in entries:
                    if entry['guid'] == last_guid or (last_date and entry['pubdate'] <= last_date):
                        break
                    new_entries.append(entry)

                # å‘é€é€šçŸ¥
                if new_entries:
                    message = [f"ğŸ“¢ {feed_title}\n{'='*30}"]
                    for idx, entry in enumerate(reversed(new_entries), 1):
                        message.append(f"{idx}. {entry['title']}\nğŸ”— {entry['link']}\n")
                    message.append(f"âœ… æ–°å¢ {len(new_entries)} æ¡å†…å®¹")

                    for chat_id in chat_ids:
                        await bot.send_message(
                            chat_id=chat_id,
                            text="\n".join(message),
                            disable_web_page_preview=False
                        )
                        await asyncio.sleep(1)

                    # æ›´æ–°çŠ¶æ€
                    status[feed_url] = {
                        'guid': entries[0]['guid'],
                        'pubdate': entries[0]['pubdate'].isoformat()
                    }
                    save_rss_status(status)  # åŒæ­¥ä¿å­˜
                return  # æˆåŠŸå¤„ç†ï¼Œé€€å‡ºé‡è¯•å¾ªç¯

        except Exception as e:
            logging.error(f"å¤„ç†æºæ—¶å‘ç”Ÿé”™è¯¯ {feed_url} (å°è¯• {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(5 * (attempt + 1))  # æŒ‡æ•°é€€é¿

    logging.error(f"å¤„ç†æº {feed_url} å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")


async def process_feed_with_semaphore(semaphore, session, feed_url, status, bot, chat_ids):
    async with semaphore:
        await process_feed(session, feed_url, status, bot, chat_ids)


async def main():
    """ä¸»å‡½æ•°"""
    # è¯»å–é…ç½®
    RSS_TOKEN = os.getenv("RSS_TOKEN")
    _TOKEN = os.getenv("_RSS")
    CHAT_IDS = os.getenv("TELEGRAM_CHAT_ID", "").split(",")

    if not RSS_TOKEN or not _TOKEN:
        logging.error("ç¼ºå°‘æœºå™¨äººTokené…ç½®")
        return

    # åˆå§‹åŒ–æœºå™¨äºº
    main_bot = Bot(token=RSS_TOKEN)
    second_bot = Bot(token=_TOKEN)
    status = load_rss_status()  # åŒæ­¥åŠ è½½

    # åˆ›å»ºä¿¡å·é‡
    semaphore = asyncio.Semaphore(5)  # é™åˆ¶å¹¶å‘æ•°ä¸º 5

    timeout = ClientTimeout(total=60) # è®¾ç½®æ€»è¶…æ—¶æ—¶é—´ä¸º60ç§’
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=100), timeout=timeout) as session: # å¢å¤§è¿æ¥æ± å¤§å°
        tasks = []
        for feed in RSS_FEEDS:
            tasks.append(process_feed_with_semaphore(semaphore, session, feed, status, main_bot, CHAT_IDS))
            await asyncio.sleep(0.5) #å»¶è¿Ÿ 0.5 ç§’

        for feed in SECOND_RSS_FEEDS:
            tasks.append(process_feed_with_semaphore(semaphore, session, feed, status, second_bot, CHAT_IDS))
            await asyncio.sleep(0.5)  # å»¶è¿Ÿ 0.5 ç§’

        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
ç”¨markdownv2å‘é€åˆ°ç”µæŠ¥æœºå™¨äºº åˆ é™¤def clean_title(title):ç›¸å…³ä»£ç  ä½¿ç”¨  python-telegram-bot  åº“çš„  escape_markdown  å‡½æ•°è‡ªåŠ¨è½¬ä¹‰ï¼šå¢åŠ å»é™¤htmlæ ‡ç­¾ï¼Œç”¨åº“è‡ªåŠ¨è½¬ä¹‰æ›¿æ¢å…¶ä»–å¤„ç†æ–¹å¼å’Œé“¾æ¥ã€‚